from featureExtraction import extract_infos
import pefile
import os
import array
import math
import pickle
from sklearn.externals import joblib
import sys
import argparse
import os, sys, shutil, time
import re
import pandas as pd
from choose_classifier import getResult
from flask import Flask, request, jsonify, render_template,abort,redirect,url_for
from werkzeug import secure_filename
from sklearn.externals import joblib
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import sklearn.ensemble as ske 
from sklearn import  tree, linear_model
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
from algorithm import algorithms
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
def cutit(s,n):
   return s[n:]

app = Flask(__name__)




@app.route('/')
def home():
    return render_template('index.html')


	
@app.route('/uploader', methods = ['GET', 'POST'])
def upload_file():
   if request.method == 'POST':
    data = pd.read_csv('./data/data1.csv', sep='|')
    X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values #now droping some coloumns as axis 1(mean coloumn) and will show the values in the rows
    y = data['legitimate'].values #values of legitimate data

    print('Researching important feature based on %i total features\n' % X.shape[1])# shape() is use in pandas to give number of row/column
    newX = []
    newY = []
    for i in X:
        a1 = []
        flag = 1
        for j in i:
            try:
                k = float(j)
                a1.append(k)
            except:
                flag = 0
                break
        if flag == 1:
            newX.append(a1)
    for i in y:
        try:
            k = float(i)
            newY.append(k)
        except:
            print('')   
    fsel = ske.ExtraTreesClassifier().fit(X, y)
    # X = newX
    # y = newY
    # model = SelectFromModel(fsel, prefit=True)
    # X_new = model.transform(X)#now features are only 9 :)
    X_new = X
    nb_features = X_new.shape[1]#will save value 13 as shape is (138047, 13) :}

    features = []

    print('%i features identified as important:' % nb_features) #as mentioned above


    #important features stored
    indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
    for f in range(nb_features):
        print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))
    
    drop_data = ['Name','md5','legitimate']
    # mean adding to the empty 'features' array the 'important features'
    for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):#[::-1] mean start with last towards first 
        if fsel.feature_importances_[f]*1000.0 > 5.0:
            features.append(data.columns[2+f])
        else:
            drop_data.append(data.columns[2+f])
    print(drop_data)
    X_new = data.drop(drop_data, axis=1).values
    X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.2)#now converting in training and testing data in 20% range 
    nb_features = len(features)
    print("Upload.....#######################################")
    f = request.files['file']
    f.save(secure_filename(f.filename))
    ##########################################
    # Load classifier
    # maxclf = joblib.load(os.path.join(os.path.dirname(os.path.realpath(__file__)),'classifier/classifier.pkl'))
    # features =pickle.loads(open(os.path.join(os.path.dirname(os.path.realpath(__file__))
                                    #    ,'classifier/features.pkl'),'r').read())
     ##########################################
    tweet =f.filename
    print("Tweats.....#######################################  " + tweet + "..#######################################")
     #########################################
    order,data = extract_infos(tweet)
    pe_features = map(lambda x: data[x], features)
    print('Features.....#######################################  ')
    j = 0
    for i in features:
        print(i + ' : ' + str(pe_features[j]))
        j = j+1
    print('..#######################################')
    print("Upload.....#######################################")
    
    malicious = 0
    legitimate = 0
    res = 1
    # clf = maxclf
    max_res = 0
    max_score = 0
    for algo in algorithms:
        clf = algorithms[algo]
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)
        print("%s : %f %%" % (algo, score*100))
    
        res1 = clf.predict(X_test)
        res = clf.predict([pe_features])[0]
        res = int(res)
        if score > max_score:
            max_score = score
            max_res = res
        print(algo + ' : ' + ['malicious', 'legitimate'][res])
        mt = confusion_matrix(y_test, res1)
        #A confusion matrix, also known as an error matrix,[4] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning.
        print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0])))*100))
        print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))
        print(res)
        if res == 1:
            legitimate = legitimate+1
        else:
            malicious = malicious+1
    if malicious > legitimate:
        res = 0
        #########################################
     #print('The file %s is %s' % (os.path.basename(sys.argv[1]),['malicious', 'legitimate'][res]))
    print('Majority result: ' + ['malicious', 'legitimate'][res])
    print('Winner Classifier result: ' + ['malicious', 'legitimate'][max_res])
    return render_template('result.html', prediction = ['malicious', 'legitimate'][max_res])



if __name__ == '__main__':
  app.run(debug = True)
